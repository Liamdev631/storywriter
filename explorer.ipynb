{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/anaconda3/envs/storywriter_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m new_index \u001b[39m=\u001b[39m GPTKnowledgeGraphIndex\u001b[39m.\u001b[39mfrom_documents(\n\u001b[1;32m     19\u001b[0m \tdocuments, \n\u001b[1;32m     20\u001b[0m \tmax_triplets_per_chunk\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     21\u001b[0m \tservice_context\u001b[39m=\u001b[39mservice_context\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms the novel about?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m query_engine \u001b[39m=\u001b[39m new_index\u001b[39m.\u001b[39;49mas_query_engine()\n\u001b[1;32m     26\u001b[0m response \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39mquery(question)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/code/storywriter/llama_index/llama_index/indices/base.py:245\u001b[0m, in \u001b[0;36mBaseGPTIndex.as_query_engine\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_query_engine\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseQueryEngine:\n\u001b[1;32m    242\u001b[0m     \u001b[39m# NOTE: lazy import\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquery_engine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretriever_query_engine\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrieverQueryEngine\n\u001b[0;32m--> 245\u001b[0m     retriever \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mas_retriever(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    247\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mretriever\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m retriever\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mservice_context\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/code/storywriter/llama_index/llama_index/indices/knowledge_graph/base.py:82\u001b[0m, in \u001b[0;36mGPTKnowledgeGraphIndex.as_retriever\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_struct\u001b[39m.\u001b[39membedding_dict) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mretriever_mode\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m     80\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mretriever_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m KGRetrieverMode\u001b[39m.\u001b[39mHYBRID\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m KGTableRetriever(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/storywriter/llama_index/llama_index/indices/knowledge_graph/retrievers.py:82\u001b[0m, in \u001b[0;36mKGTableRetriever.__init__\u001b[0;34m(self, index, query_keyword_extract_template, max_keywords_per_query, num_chunks_per_query, include_text, retriever_mode, similarity_top_k, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     index: GPTKnowledgeGraphIndex,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     79\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(index, GPTKnowledgeGraphIndex)\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m=\u001b[39m index\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index\u001b[39m.\u001b[39mservice_context\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '20'\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "from llama_index.llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
    "from llama_index.llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "from langchain.llms import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "documents = SimpleDirectoryReader('output/').load_data()\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003') # type: ignore\n",
    "llm_predictor = LLMPredictor(llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
    "\n",
    "new_index = GPTKnowledgeGraphIndex.from_documents(\n",
    "\tdocuments, \n",
    "\tmax_triplets_per_chunk=2,\n",
    "\tservice_context=service_context\n",
    ")\n",
    "\n",
    "question = \"What's the novel about?\"\n",
    "query_engine = new_index.as_query_engine()\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storywriter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
